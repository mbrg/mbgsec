---
title: "AI Lab Watch"
tags:
   - Safety Research
   - Risk Assessment
   - AI Safety
   - Ethics in AI
   - Machine Learning Governance
link: https://ailabwatch.org
date: 2025-07-20
description: "AI Lab Watch evaluates safety practices among major AI companies, revealing varying degrees of commitment to risk assessment, safety research, and misuse prevention. Anthropic leads with a 27% overall score, excelling in safety research (68%) and risk assessment (44%). DeepMind follows at 21%, while OpenAI holds 17%. Notably, many firms show minimal attention to misuse prevention and extreme security. The report emphasizes the necessity for these companies to enhance safety protocols amid rising AI capabilities, suggesting deeper collaboration and transparency in safety initiatives."
---
{% raw %}

### [AI Lab Watch](https://ailabwatch.org/)

[**Categories**](https://ailabwatch.org/categories) [**Companies**](https://ailabwatch.org/companies) [**Resources**](https://ailabwatch.org/resources) [**Blog**](https://ailabwatch.substack.com/) [**About**](https://ailabwatch.org/about)

|  | [![Anthropic](https://ailabwatch.org/logos/anthropic.svg)](https://ailabwatch.org/companies/anthropic) | [![DeepMind](https://ailabwatch.org/logos/DeepMind_new_logo.svg)](https://ailabwatch.org/companies/deepmind) | [![OpenAI](https://ailabwatch.org/logos/openai.svg)](https://ailabwatch.org/companies/openai)[\*](https://ailabwatch.org/openai-asterisk) | [![Meta](https://ailabwatch.org/logos/meta.svg)](https://ailabwatch.org/companies/meta) | [![xAI](https://ailabwatch.org/logos/XAI_Logo.svg)](https://ailabwatch.org/companies/xai) | [![Microsoft](https://ailabwatch.org/logos/microsoft.svg)](https://ailabwatch.org/companies/microsoft) | [![DeepSeek](https://ailabwatch.org/logos/deepseek-logo-icon.svg)](https://ailabwatch.org/companies/deepseek) |
| --- | --- | --- | --- | --- | --- | --- | --- |
| [Weighted score](https://ailabwatch.org/categories) | [27%](https://ailabwatch.org/companies/anthropic) | [21%](https://ailabwatch.org/companies/deepmind) | [17%](https://ailabwatch.org/companies/openai) | [5%](https://ailabwatch.org/companies/meta) | [3%](https://ailabwatch.org/companies/xai) | [3%](https://ailabwatch.org/companies/microsoft) | [1%](https://ailabwatch.org/companies/deepseek) |
| --- | --- | --- | --- | --- | --- | --- | --- |
| [Risk assessment](https://ailabwatch.org/categories/risk-assessment) | [44%](https://ailabwatch.org/cell/anthropic/risk-assessment) | [29%](https://ailabwatch.org/cell/deepmind/risk-assessment) | [32%](https://ailabwatch.org/cell/openai/risk-assessment) | [1%](https://ailabwatch.org/cell/meta/risk-assessment) | [1%](https://ailabwatch.org/cell/xai/risk-assessment) | [1%](https://ailabwatch.org/cell/microsoft/risk-assessment) | [0%](https://ailabwatch.org/cell/deepseek/risk-assessment) | [27% weight](https://ailabwatch.org/categories/risk-assessment) |
| [Scheming risk prevention](https://ailabwatch.org/categories/scheming) | [3%](https://ailabwatch.org/cell/anthropic/scheming) | [8%](https://ailabwatch.org/cell/deepmind/scheming) | [2%](https://ailabwatch.org/cell/openai/scheming) | [2%](https://ailabwatch.org/cell/meta/scheming) | [2%](https://ailabwatch.org/cell/xai/scheming) | [2%](https://ailabwatch.org/cell/microsoft/scheming) | [2%](https://ailabwatch.org/cell/deepseek/scheming) | [21% weight](https://ailabwatch.org/categories/scheming) |
| [Boosting safety research](https://ailabwatch.org/categories/safety-research) | [68%](https://ailabwatch.org/cell/anthropic/safety-research) | [56%](https://ailabwatch.org/cell/deepmind/safety-research) | [37%](https://ailabwatch.org/cell/openai/safety-research) | [28%](https://ailabwatch.org/cell/meta/safety-research) | [0%](https://ailabwatch.org/cell/xai/safety-research) | [15%](https://ailabwatch.org/cell/microsoft/safety-research) | [8%](https://ailabwatch.org/cell/deepseek/safety-research) | [14% weight](https://ailabwatch.org/categories/safety-research) |
| [Misuse prevention](https://ailabwatch.org/categories/misuse) | [12%](https://ailabwatch.org/cell/anthropic/misuse) | [4%](https://ailabwatch.org/cell/deepmind/misuse) | [5%](https://ailabwatch.org/cell/openai/misuse) | [0%](https://ailabwatch.org/cell/meta/misuse) | [0%](https://ailabwatch.org/cell/xai/misuse) | [0%](https://ailabwatch.org/cell/microsoft/misuse) | [0%](https://ailabwatch.org/cell/deepseek/misuse) | [12% weight](https://ailabwatch.org/categories/misuse) |
| [Prep for extreme security](https://ailabwatch.org/categories/security) | [3%](https://ailabwatch.org/cell/anthropic/security) | [5%](https://ailabwatch.org/cell/deepmind/security) | [0%](https://ailabwatch.org/cell/openai/security) | [0%](https://ailabwatch.org/cell/meta/security) | [0%](https://ailabwatch.org/cell/xai/security) | [0%](https://ailabwatch.org/cell/microsoft/security) | [0%](https://ailabwatch.org/cell/deepseek/security) | [12% weight](https://ailabwatch.org/categories/security) |
| [Risk info sharing](https://ailabwatch.org/categories/information-sharing) | [35%](https://ailabwatch.org/cell/anthropic/information-sharing) | [13%](https://ailabwatch.org/cell/deepmind/information-sharing) | [32%](https://ailabwatch.org/cell/openai/information-sharing) | [0%](https://ailabwatch.org/cell/meta/information-sharing) | [28%](https://ailabwatch.org/cell/xai/information-sharing) | [0%](https://ailabwatch.org/cell/microsoft/information-sharing) | [0%](https://ailabwatch.org/cell/deepseek/information-sharing) | [8% weight](https://ailabwatch.org/categories/information-sharing) |
| [Planning](https://ailabwatch.org/categories/planning) | [14%](https://ailabwatch.org/cell/anthropic/planning) | [26%](https://ailabwatch.org/cell/deepmind/planning) | [0%](https://ailabwatch.org/cell/openai/planning) | [0%](https://ailabwatch.org/cell/meta/planning) | [0%](https://ailabwatch.org/cell/xai/planning) | [1%](https://ailabwatch.org/cell/microsoft/planning) | [0%](https://ailabwatch.org/cell/deepseek/planning) | [6% weight](https://ailabwatch.org/categories/planning) |

_Up to date as of July 13_

Overall score

[![Anthropic](https://ailabwatch.org/logos/anthropic.svg)\\
\\
Anthropic\\
\\
27\\
\\
%](https://ailabwatch.org/companies/anthropic) [![DeepMind](https://ailabwatch.org/logos/DeepMind_new_logo.svg)\\
\\
DeepMind\\
\\
21\\
\\
%](https://ailabwatch.org/companies/deepmind) [![OpenAI](https://ailabwatch.org/logos/openai.svg)\\
\\
OpenAI\\
\\
17\\
\\
%](https://ailabwatch.org/companies/openai) [![Meta](https://ailabwatch.org/logos/meta.svg)\\
\\
Meta\\
\\
5\\
\\
%](https://ailabwatch.org/companies/meta) [![xAI](https://ailabwatch.org/logos/XAI_Logo.svg)\\
\\
xAI\\
\\
3\\
\\
%](https://ailabwatch.org/companies/xai) [![Microsoft](https://ailabwatch.org/logos/microsoft.svg)\\
\\
Microsoft\\
\\
3\\
\\
%](https://ailabwatch.org/companies/microsoft) [![DeepSeek](https://ailabwatch.org/logos/deepseek-logo-icon.svg)\\
\\
DeepSeek\\
\\
1\\
\\
%](https://ailabwatch.org/companies/deepseek)

[Risk assessment](https://ailabwatch.org/categories/risk-assessment)

AI companies should do model evals and uplift experiments to determine whether models have dangerous capabilities or how close they are. They should also prepare to check whether models will act well in high-stakes situations.

| [![Anthropic](https://ailabwatch.org/logos/anthropic.svg)](https://ailabwatch.org/companies/anthropic) | [![DeepMind](https://ailabwatch.org/logos/DeepMind_new_logo.svg)](https://ailabwatch.org/companies/deepmind) | [![OpenAI](https://ailabwatch.org/logos/openai.svg)](https://ailabwatch.org/companies/openai) | [![Meta](https://ailabwatch.org/logos/meta.svg)](https://ailabwatch.org/companies/meta) | [![xAI](https://ailabwatch.org/logos/XAI_Logo.svg)](https://ailabwatch.org/companies/xai) | [![Microsoft](https://ailabwatch.org/logos/microsoft.svg)](https://ailabwatch.org/companies/microsoft) | [![DeepSeek](https://ailabwatch.org/logos/deepseek-logo-icon.svg)](https://ailabwatch.org/companies/deepseek) |  |
| --- | --- | --- | --- | --- | --- | --- | --- |
| [44\<br>\<br>%](https://ailabwatch.org/cell/anthropic/risk-assessment) | [29\<br>\<br>%](https://ailabwatch.org/cell/deepmind/risk-assessment) | [32\<br>\<br>%](https://ailabwatch.org/cell/openai/risk-assessment) | [1\<br>\<br>%](https://ailabwatch.org/cell/meta/risk-assessment) | [1\<br>\<br>%](https://ailabwatch.org/cell/xai/risk-assessment) | [1\<br>\<br>%](https://ailabwatch.org/cell/microsoft/risk-assessment) | [0\<br>\<br>%](https://ailabwatch.org/cell/deepseek/risk-assessment) |

[Scheming risk prevention](https://ailabwatch.org/categories/scheming)

AIs show signs that if they were more capable, they would sometimes [scheme](https://joecarlsmith.com/2023/11/15/new-report-scheming-ais-will-ais-fake-alignment-during-training-in-order-to-get-power), i.e. fake alignment and subvert safety measures in order to gain power. AI companies should prepare for risks from models scheming, especially during internal deployment: if they can't reliably prevent scheming, they should prepare to catch some schemers and deploy potentially scheming models safely.

| [![Anthropic](https://ailabwatch.org/logos/anthropic.svg)](https://ailabwatch.org/companies/anthropic) | [![DeepMind](https://ailabwatch.org/logos/DeepMind_new_logo.svg)](https://ailabwatch.org/companies/deepmind) | [![OpenAI](https://ailabwatch.org/logos/openai.svg)](https://ailabwatch.org/companies/openai) | [![Meta](https://ailabwatch.org/logos/meta.svg)](https://ailabwatch.org/companies/meta) | [![xAI](https://ailabwatch.org/logos/XAI_Logo.svg)](https://ailabwatch.org/companies/xai) | [![Microsoft](https://ailabwatch.org/logos/microsoft.svg)](https://ailabwatch.org/companies/microsoft) | [![DeepSeek](https://ailabwatch.org/logos/deepseek-logo-icon.svg)](https://ailabwatch.org/companies/deepseek) |  |
| --- | --- | --- | --- | --- | --- | --- | --- |
| [3\<br>\<br>%](https://ailabwatch.org/cell/anthropic/scheming) | [8\<br>\<br>%](https://ailabwatch.org/cell/deepmind/scheming) | [2\<br>\<br>%](https://ailabwatch.org/cell/openai/scheming) | [2\<br>\<br>%](https://ailabwatch.org/cell/meta/scheming) | [2\<br>\<br>%](https://ailabwatch.org/cell/xai/scheming) | [2\<br>\<br>%](https://ailabwatch.org/cell/microsoft/scheming) | [2\<br>\<br>%](https://ailabwatch.org/cell/deepseek/scheming) |

[Boosting safety research](https://ailabwatch.org/categories/safety-research)

AI companies should do (extreme-risk-focused) safety research, and they should publish it to boost safety at _other_ AI companies. Additionally, they should assist external safety researchers by sharing deep model access and mentoring.

| [![Anthropic](https://ailabwatch.org/logos/anthropic.svg)](https://ailabwatch.org/companies/anthropic) | [![DeepMind](https://ailabwatch.org/logos/DeepMind_new_logo.svg)](https://ailabwatch.org/companies/deepmind) | [![OpenAI](https://ailabwatch.org/logos/openai.svg)](https://ailabwatch.org/companies/openai) | [![Meta](https://ailabwatch.org/logos/meta.svg)](https://ailabwatch.org/companies/meta) | [![xAI](https://ailabwatch.org/logos/XAI_Logo.svg)](https://ailabwatch.org/companies/xai) | [![Microsoft](https://ailabwatch.org/logos/microsoft.svg)](https://ailabwatch.org/companies/microsoft) | [![DeepSeek](https://ailabwatch.org/logos/deepseek-logo-icon.svg)](https://ailabwatch.org/companies/deepseek) |  |
| --- | --- | --- | --- | --- | --- | --- | --- |
| [68\<br>\<br>%](https://ailabwatch.org/cell/anthropic/safety-research) | [56\<br>\<br>%](https://ailabwatch.org/cell/deepmind/safety-research) | [37\<br>\<br>%](https://ailabwatch.org/cell/openai/safety-research) | [28\<br>\<br>%](https://ailabwatch.org/cell/meta/safety-research) | [0\<br>\<br>%](https://ailabwatch.org/cell/xai/safety-research) | [15\<br>\<br>%](https://ailabwatch.org/cell/microsoft/safety-research) | [8\<br>\<br>%](https://ailabwatch.org/cell/deepseek/safety-research) |

[Misuse prevention](https://ailabwatch.org/categories/misuse)

AI companies should prepare to prevent catastrophic misuse for deployments via API, once models are capable of enabling catastrophic harm.

| [![Anthropic](https://ailabwatch.org/logos/anthropic.svg)](https://ailabwatch.org/companies/anthropic) | [![DeepMind](https://ailabwatch.org/logos/DeepMind_new_logo.svg)](https://ailabwatch.org/companies/deepmind) | [![OpenAI](https://ailabwatch.org/logos/openai.svg)](https://ailabwatch.org/companies/openai) | [![Meta](https://ailabwatch.org/logos/meta.svg)](https://ailabwatch.org/companies/meta) | [![xAI](https://ailabwatch.org/logos/XAI_Logo.svg)](https://ailabwatch.org/companies/xai) | [![Microsoft](https://ailabwatch.org/logos/microsoft.svg)](https://ailabwatch.org/companies/microsoft) | [![DeepSeek](https://ailabwatch.org/logos/deepseek-logo-icon.svg)](https://ailabwatch.org/companies/deepseek) |  |
| --- | --- | --- | --- | --- | --- | --- | --- |
| [12\<br>\<br>%](https://ailabwatch.org/cell/anthropic/misuse) | [4\<br>\<br>%](https://ailabwatch.org/cell/deepmind/misuse) | [5\<br>\<br>%](https://ailabwatch.org/cell/openai/misuse) | [0\<br>\<br>%](https://ailabwatch.org/cell/meta/misuse) | [0\<br>\<br>%](https://ailabwatch.org/cell/xai/misuse) | [0\<br>\<br>%](https://ailabwatch.org/cell/microsoft/misuse) | [0\<br>\<br>%](https://ailabwatch.org/cell/deepseek/misuse) |

[Prep for extreme security](https://ailabwatch.org/categories/security)

AI companies should prepare to protect model weights and code by the time AI massively boosts R&D, even [from](https://www.rand.org/pubs/research_briefs/RBA2849-1.html) top-priority operations by the top cyber-capable institutions.

| [![Anthropic](https://ailabwatch.org/logos/anthropic.svg)](https://ailabwatch.org/companies/anthropic) | [![DeepMind](https://ailabwatch.org/logos/DeepMind_new_logo.svg)](https://ailabwatch.org/companies/deepmind) | [![OpenAI](https://ailabwatch.org/logos/openai.svg)](https://ailabwatch.org/companies/openai) | [![Meta](https://ailabwatch.org/logos/meta.svg)](https://ailabwatch.org/companies/meta) | [![xAI](https://ailabwatch.org/logos/XAI_Logo.svg)](https://ailabwatch.org/companies/xai) | [![Microsoft](https://ailabwatch.org/logos/microsoft.svg)](https://ailabwatch.org/companies/microsoft) | [![DeepSeek](https://ailabwatch.org/logos/deepseek-logo-icon.svg)](https://ailabwatch.org/companies/deepseek) |  |
| --- | --- | --- | --- | --- | --- | --- | --- |
| [3\<br>\<br>%](https://ailabwatch.org/cell/anthropic/security) | [5\<br>\<br>%](https://ailabwatch.org/cell/deepmind/security) | [0\<br>\<br>%](https://ailabwatch.org/cell/openai/security) | [0\<br>\<br>%](https://ailabwatch.org/cell/meta/security) | [0\<br>\<br>%](https://ailabwatch.org/cell/xai/security) | [0\<br>\<br>%](https://ailabwatch.org/cell/microsoft/security) | [0\<br>\<br>%](https://ailabwatch.org/cell/deepseek/security) |

[Risk info sharing](https://ailabwatch.org/categories/information-sharing)

AI companies should share information on incidents, risks, and capabilities — but not share some capabilities research.

| [![Anthropic](https://ailabwatch.org/logos/anthropic.svg)](https://ailabwatch.org/companies/anthropic) | [![DeepMind](https://ailabwatch.org/logos/DeepMind_new_logo.svg)](https://ailabwatch.org/companies/deepmind) | [![OpenAI](https://ailabwatch.org/logos/openai.svg)](https://ailabwatch.org/companies/openai) | [![Meta](https://ailabwatch.org/logos/meta.svg)](https://ailabwatch.org/companies/meta) | [![xAI](https://ailabwatch.org/logos/XAI_Logo.svg)](https://ailabwatch.org/companies/xai) | [![Microsoft](https://ailabwatch.org/logos/microsoft.svg)](https://ailabwatch.org/companies/microsoft) | [![DeepSeek](https://ailabwatch.org/logos/deepseek-logo-icon.svg)](https://ailabwatch.org/companies/deepseek) |  |
| --- | --- | --- | --- | --- | --- | --- | --- |
| [35\<br>\<br>%](https://ailabwatch.org/cell/anthropic/information-sharing) | [13\<br>\<br>%](https://ailabwatch.org/cell/deepmind/information-sharing) | [32\<br>\<br>%](https://ailabwatch.org/cell/openai/information-sharing) | [0\<br>\<br>%](https://ailabwatch.org/cell/meta/information-sharing) | [28\<br>\<br>%](https://ailabwatch.org/cell/xai/information-sharing) | [0\<br>\<br>%](https://ailabwatch.org/cell/microsoft/information-sharing) | [0\<br>\<br>%](https://ailabwatch.org/cell/deepseek/information-sharing) |

[Planning](https://ailabwatch.org/categories/planning)

AI companies should plan for the possibility that dangerous capabilities appear soon and safety isn't easy: both for evaluating and improving safety of their systems and for using their systems to make the world safer.

| [![Anthropic](https://ailabwatch.org/logos/anthropic.svg)](https://ailabwatch.org/companies/anthropic) | [![DeepMind](https://ailabwatch.org/logos/DeepMind_new_logo.svg)](https://ailabwatch.org/companies/deepmind) | [![OpenAI](https://ailabwatch.org/logos/openai.svg)](https://ailabwatch.org/companies/openai) | [![Meta](https://ailabwatch.org/logos/meta.svg)](https://ailabwatch.org/companies/meta) | [![xAI](https://ailabwatch.org/logos/XAI_Logo.svg)](https://ailabwatch.org/companies/xai) | [![Microsoft](https://ailabwatch.org/logos/microsoft.svg)](https://ailabwatch.org/companies/microsoft) | [![DeepSeek](https://ailabwatch.org/logos/deepseek-logo-icon.svg)](https://ailabwatch.org/companies/deepseek) |  |
| --- | --- | --- | --- | --- | --- | --- | --- |
| [14\<br>\<br>%](https://ailabwatch.org/cell/anthropic/planning) | [26\<br>\<br>%](https://ailabwatch.org/cell/deepmind/planning) | [0\<br>\<br>%](https://ailabwatch.org/cell/openai/planning) | [0\<br>\<br>%](https://ailabwatch.org/cell/meta/planning) | [0\<br>\<br>%](https://ailabwatch.org/cell/xai/planning) | [1\<br>\<br>%](https://ailabwatch.org/cell/microsoft/planning) | [0\<br>\<br>%](https://ailabwatch.org/cell/deepseek/planning) |

I'm Zach Stein-Perlman. I'm worried about future powerful Als causing an existential catastrophe. Here at AI Lab Watch, I track what AI companies are doing in terms of safety.

In this scorecard, I collect actions AI companies can take to improve safety and public information on what they're doing.

**Click on a column or cell to see details about a company; click on a row to see details about a category.**

Criteria are grouped into categories; both are weighted by how important they currently are for safety and how much signal the criteria provide/capture. These criteria are not exhaustive; some important variables are hard to measure. I endorse the words more than the numbers; the scores on particular criteria and the weights are largely judgment calls.

In addition to the scorecard, I write [blogposts](https://ailabwatch.substack.com/) on what AI companies should do and what they are doing, and I maintain [resources](https://ailabwatch.org/resources) with information on these topics.

I'm Zach Stein-Perlman. I'm worried about future powerful Als causing an existential catastrophe. I track what AI companies are doing in terms of safety.

For some details on what AI companies should do and what they are doing in terms of safety, click around this scorecard. Or check out the articles below, the rest of my [blog](https://ailabwatch.substack.com/), or the [resources](https://ailabwatch.org/resources) I maintain.

## Featured posts

[What AI companies should do →](https://ailabwatch.substack.com/p/what-ai-companies-should-do) [AI companies' eval reports mostly don't support their claims →](https://ailabwatch.substack.com/p/ai-companies-eval-reports-mostly) [AI companies' commitments →](https://ailabwatch.org/resources/commitments) [AI companies' integrity incidents →](https://ailabwatch.org/resources/integrity)

[Subscribe to the blog](https://ailabwatch.substack.com/)
{% endraw %}
