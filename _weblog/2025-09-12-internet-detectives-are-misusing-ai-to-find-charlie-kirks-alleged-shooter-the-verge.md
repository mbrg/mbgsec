---
date: '2025-09-15'
description: Online users are leveraging AI to enhance FBI-released images of a person
  of interest in the Charlie Kirk shooting. However, the AI tools used merely infer
  details without uncovering accurate information, leading to misleading representations.
  In previous instances, AI-generated images have inaccurately modified features,
  underscoring the technology's inherent risks in critical scenarios like manhunts.
  This trend raises concerns over reliance on AI for evidence, emphasizing a potential
  need for clearer guidelines on AI's application in public safety efforts.
link: /archive/2025-09-12-internet-detectives-are-misusing-ai-to-find-charlie-kirks-alleged-shooter-the-verge
tags:
- AI
- social media
- FBI
- photo enhancement
- misinformation
- weblog
title: "Internet detectives are misusing AI to find Charlie Kirk\u2019s alleged shooter\
  \ \u25C6 The Verge"
type: weblog
---
{% raw %}

This says so much about how we think about AI and computer-generate stuff in general. Just because its plausible doesn't mean its true.

---

> Many AI-generated photo variations were posted under the original images, some apparently created with X’s own Grok bot, others with tools like ChatGPT. They vary in plausibility, though some are obviously off, like an “AI-based textual rendering” showing a clearly different shirt and [Gigachad-level chin](https://x.com/JohnNosta/status/1966238703917084899). The images are ostensibly supposed to help people find the person of interest, although they’re also eye-grabbing ways to get likes and reposts.

"Gigachad-level chin" lol

{% endraw %}
